# ğŸ§  Full-Stack ML + RAG Chatbot App

A full-stack Streamlit application that combines **real-time machine learning predictions** with a **RAG (Retrieval-Augmented Generation) chatbot** powered by **Groq's LLaMA models** and local documentation.

---

## ğŸ“Œ Project Overview

This project simulates live customer interaction data and performs:
- **Real-time customer value prediction** using a trained ML model.
- **Conversational Q&A** over local documentation (Python/ML/SQL PDFs) using FAISS + HuggingFace embeddings + LangChain + Groq LLM.

The app has **two main features**:
- **ğŸ“Š Real-Time Dashboard**: Predicts whether a simulated customer has *high* or *low* value using user behavior.
- **ğŸ’¬ RAG Chatbot**: Allows users to query documentation using a Groq-powered LLM backed by a local FAISS index.

---

## âš™ï¸ Setup & Execution Instructions


### âœ… 1. Install dependencies

Make sure Python 3.8+ is installed.

```bash
pip install -r requirements.txt
```

**`requirements.txt` includes**:
- streamlit, scikit-learn, pandas, numpy, matplotlib
- sentence-transformers, langchain, faiss-cpu, PyMuPDF
- langchain-groq, joblib, imbalanced-learn

### âœ… 2. Place your PDF documents

Add your documentation files (e.g. Python, ML, SQL) inside the `data/` folder.

```
project-root/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ your_docs.pdf
```

### âœ… 3. Set your Groq API key

You can set it via environment variable:

```bash
export GROQ_API_KEY="your-key"    # On Linux/macOS
set GROQ_API_KEY=your-key         # On Windows
```

Or edit the hardcoded key inside `rag_chatbot.py`.

### âœ… 4. Run the app

```bash
streamlit run app.py
```

---

## ğŸ§  Model & Tool Explanation

### ğŸ“ˆ Machine Learning Model

- **Model Type**: `RandomForestClassifier` (scikit-learn)
- **Training Data**: Simulated customer behavior including:
  - Product type, price, number of clicks
  - Device type, region, age, session time
  - Whether the user is returning
- **Target**: Binary classification â€“ "high" vs "low" customer value
- **Preprocessing**:
  - One-hot encoding for categorical variables
  - Resampling via `RandomOverSampler` to handle class imbalance
- **Saved Model**: `resampled_customer_value_model.joblib`

### ğŸ¤– RAG Chatbot (LangChain + Groq + FAISS)

- **Embedding Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Text Chunking**: `RecursiveCharacterTextSplitter` with 500-char chunks
- **Vector Store**: `FAISS` (stored in `faiss_index/`)
- **LLM**: Groq-hosted LLaMA 3.3-70B (`llama-3.3-70b-versatile`) via `langchain-groq`
- **Pipeline**:
  - Load PDFs using `PyMuPDF`
  - Split into chunks and embed
  - Store in FAISS and retrieve top-k relevant chunks
  - Generate answer using LLM

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ app.py                      # Main Streamlit interface
â”œâ”€â”€ rag_chatbot.py             # RAG chatbot logic
â”œâ”€â”€ model_training.py          # (your training code)
â”œâ”€â”€ resampled_customer_value_model.joblib
â”œâ”€â”€ realtime_sales.csv         # Simulated dataset
â”œâ”€â”€ faiss_index/               # Auto-generated vector index
â”œâ”€â”€ data/                      # Folder for documentation PDFs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Visualization Example

- Live data appears in a growing table
- Bar chart shows number of high/low value predictions over time
- Sidebar controls for simulation speed, filters, and row limits

---

## ğŸ’¡ Use Cases

- Teaching/learning full-stack ML deployment
- Exploring LangChain-based RAG pipelines
- Building fast LLM apps with Groq
- Simulating real-time data dashboards

---

## ğŸ“„ License

MIT License â€” Free to use, modify, and share.

---

## ğŸ™ Acknowledgements

- [Streamlit](https://streamlit.io/)
- [scikit-learn](https://scikit-learn.org/)
- [LangChain](https://www.langchain.com/)
- [Groq](https://groq.com/)
- [HuggingFace](https://huggingface.co/)
